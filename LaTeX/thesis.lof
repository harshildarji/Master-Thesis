\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.1}{\ignorespaces Hand-written digits}}{1}{figure.caption.7}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.2}{\ignorespaces Flowchart for the pruning experiment}}{4}{figure.caption.8}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.1}{\ignorespaces Single Layer Perceptron}}{9}{figure.caption.9}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.2}{\ignorespaces Rectified Linear Unit}}{11}{figure.caption.11}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.3}{\ignorespaces Sigmoid activation function}}{11}{figure.caption.13}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.4}{\ignorespaces Tanh activation function}}{12}{figure.caption.15}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.5}{\ignorespaces Artificial Neural Network}}{15}{figure.caption.16}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.6}{\ignorespaces Backpropagation in a neural network}}{16}{figure.caption.17}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.7}{\ignorespaces Backpropagating error from output layer}}{17}{figure.caption.18}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.8}{\ignorespaces Backpropagating error from $h_1$}}{18}{figure.caption.19}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.9}{\ignorespaces Backpropagating error from $h_2$}}{18}{figure.caption.20}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.10}{\ignorespaces A simple Recurrent Network}}{20}{figure.caption.21}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.11}{\ignorespaces A Recurrent Neural Network unfolded in time}}{20}{figure.caption.22}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.12}{\ignorespaces Internal structure of a single hidden unit in a standard RNN}}{21}{figure.caption.24}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.13}{\ignorespaces Backpropagation Through Time}}{22}{figure.caption.25}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.14}{\ignorespaces Internal structure of a single hidden unit in an LSTM}}{25}{figure.caption.26}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.15}{\ignorespaces Internal structure of a single hidden unit in a GRU}}{27}{figure.caption.27}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.1}{\ignorespaces Flow diagram to generate Reber grammar sequences}}{36}{figure.caption.38}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.2}{\ignorespaces Validity and number of strings in train-test dataset}}{37}{figure.caption.40}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.3}{\ignorespaces String length distribution of entire dataset}}{38}{figure.caption.42}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.4}{\ignorespaces String length distribution of train dataset}}{39}{figure.caption.43}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.5}{\ignorespaces String length distribution of test dataset}}{39}{figure.caption.44}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.1}{\ignorespaces Training recurrent networks}}{40}{figure.caption.45}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.2}{\ignorespaces Generating embeddings of example input sequence}}{41}{figure.caption.46}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.3}{\ignorespaces Visualization of a single recurrent layer}}{42}{figure.caption.47}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.4}{\ignorespaces Visualization of linear layer's connection to output layer}}{43}{figure.caption.48}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.5}{\ignorespaces Simultaneous and individual pruning of i2h and h2h weights}}{44}{figure.caption.50}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.6}{\ignorespaces Two densely connected linear layers}}{45}{figure.caption.51}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.7}{\ignorespaces Two densely connected linear layers after pruning}}{46}{figure.caption.52}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.8}{\ignorespaces Undirected graph containing $5$ nodes}}{48}{figure.caption.54}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.9}{\ignorespaces Directed graph containing $5$ nodes}}{48}{figure.caption.55}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.10}{\ignorespaces Neural architecture generated using the layer indexing}}{49}{figure.caption.57}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.11}{\ignorespaces Complete model with recurrent architecture generated from a random graph}}{50}{figure.caption.58}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.1}{\ignorespaces RNN\_Tanh base model performance}}{52}{figure.caption.59}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.2}{\ignorespaces RNN\_ReLU base model performance}}{53}{figure.caption.60}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.3}{\ignorespaces LSTM base model performance}}{53}{figure.caption.61}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.4}{\ignorespaces GRU base model performance}}{54}{figure.caption.62}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.5}{\ignorespaces RNN\_Tanh base model performance after pruning}}{55}{figure.caption.63}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.6}{\ignorespaces RNN\_Tanh base model performance regain after pruning}}{55}{figure.caption.64}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.7}{\ignorespaces RNN\_ReLU base model performance after pruning}}{56}{figure.caption.65}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.8}{\ignorespaces RNN\_ReLU base model performance regain after pruning}}{56}{figure.caption.66}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.9}{\ignorespaces LSTM base model performance after pruning}}{57}{figure.caption.67}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.10}{\ignorespaces LSTM base model performance regain after pruning}}{57}{figure.caption.68}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.11}{\ignorespaces GRU base model performance after pruning}}{58}{figure.caption.69}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.12}{\ignorespaces GRU base model performance regain after pruning}}{58}{figure.caption.70}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.13}{\ignorespaces RNN\_Tanh base model performance after pruning i2h weights}}{59}{figure.caption.71}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.14}{\ignorespaces RNN\_Tanh base model performance regain after pruning i2h weights}}{60}{figure.caption.72}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.15}{\ignorespaces RNN\_ReLU base model performance after pruning i2h weights}}{60}{figure.caption.73}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.16}{\ignorespaces RNN\_ReLU base model performance regain after pruning i2h weights}}{61}{figure.caption.74}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.17}{\ignorespaces LSTM base model performance after pruning i2h weights}}{61}{figure.caption.75}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.18}{\ignorespaces LSTM base model performance regain after pruning i2h weights}}{62}{figure.caption.76}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.19}{\ignorespaces GRU base model performance after pruning i2h weights}}{62}{figure.caption.77}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.20}{\ignorespaces GRU base model performance regain after pruning i2h weights}}{63}{figure.caption.78}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.21}{\ignorespaces RNN\_Tanh base model performance after pruning h2h weights}}{64}{figure.caption.79}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.22}{\ignorespaces RNN\_Tanh base model performance regain after pruning h2h weights}}{64}{figure.caption.80}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.23}{\ignorespaces RNN\_ReLU base model performance after pruning h2h weights}}{65}{figure.caption.81}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.24}{\ignorespaces RNN\_ReLU base model performance regain after pruning h2h weights}}{65}{figure.caption.82}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.25}{\ignorespaces LSTM base model performance after pruning h2h weights}}{66}{figure.caption.83}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.26}{\ignorespaces LSTM base model performance regain after pruning h2h weights}}{66}{figure.caption.84}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.27}{\ignorespaces GRU base model performance after pruning h2h weights}}{67}{figure.caption.85}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.28}{\ignorespaces GRU base model performance regain after pruning h2h weights}}{67}{figure.caption.86}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.29}{\ignorespaces Performance of the WS and BA based random structure RNN\_Tanh}}{68}{figure.caption.87}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.30}{\ignorespaces Performance of the WS and BA based random structure RNN\_ReLU}}{69}{figure.caption.89}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.31}{\ignorespaces Performance of the WS and BA based random structure LSTM}}{70}{figure.caption.91}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.32}{\ignorespaces Performance of the WS and BA based random structure GRU}}{71}{figure.caption.93}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {C.1}{\ignorespaces Comparison of basic graph properties and number of layers in WS and BA based RNN\_Tanh models}}{81}{figure.caption.100}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {C.2}{\ignorespaces Correlation between test accuracy of RNN\_Tanh and its different graph and recurrent network properties - 1}}{82}{figure.caption.101}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {C.3}{\ignorespaces Comparison of basic graph properties and number of layers in WS and BA based RNN\_ReLU models}}{83}{figure.caption.102}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {C.4}{\ignorespaces Correlation between test accuracy of RNN\_ReLU and its different graph and recurrent network properties - 1}}{84}{figure.caption.103}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {C.5}{\ignorespaces Comparison of basic graph properties and number of layers in WS and BA based LSTM models}}{85}{figure.caption.104}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {C.6}{\ignorespaces Correlation between test accuracy of LSTM and its different graph and recurrent network properties - 1}}{86}{figure.caption.105}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {C.7}{\ignorespaces Correlation between test accuracy of LSTM and its different graph and recurrent network properties - 2}}{87}{figure.caption.106}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {C.8}{\ignorespaces Comparison of basic graph properties and number of layers in WS and BA based GRU models}}{88}{figure.caption.107}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {C.9}{\ignorespaces Correlation between test accuracy of GRU and its different graph and recurrent network properties - 1}}{89}{figure.caption.108}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {C.10}{\ignorespaces Correlation between test accuracy of GRU and its different graph and recurrent network properties - 2}}{90}{figure.caption.109}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
