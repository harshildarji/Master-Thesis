\chapter{Related works}\label{chap:literature}

A few researchers have worked on pruning to induce sparsity in Recurrent Neural Networks in the past couple of decades. Apart from that, to our knowledge, only one paper explains the process of generating Sparse Neural Networks from randomly generated graphs. This section will summarize these researches to have an idea about what has been done so far.

In September 1994, Lee et al. in \cite{lee_pruning} published one of the first pruning techniques in terms of Recurrent Neural Networks in which authors apply pruning to trained recurrent neural nets. They follow the train, prune, and retrain approach where they train an extensive network on grammar dataset, apply pruning on the trained network, and retrain on the same training set until either satisfactory performance is achieved or the network no longer converge. Since the authors use the incremental training approach, there is no need for the network to use the entire training set, meaning the network can achieve satisfactory achievement using only a part of the training set. After each pruning and retraining cycle, the network would require even less data than the initial network. Based on their experiments' results, the authors claim that ``our pruning/retraining algorithm is an effective tool for improving the generalization performance". According to the authors, this improved performance is due to the reduced size of the network.

In June 2017, Han et al. propose a Recurrent Self-Organising Neural Network (RSONN) in \cite{han_pruning}. To self-organize the RNNs' structure, they use Adaptive Growing and Pruning Algorithm (AGPA). This algorithm works by adding or pruning the hidden neurons based on their competitiveness during the training phase. Authors verify their approach on various benchmark datasets to compare against existing approaches. Based on the experiment results, the authors claim that the proposed RSONN effectively simplifies the network structure and performs better than some exiting methods. Authors compare their performance against various neural network models (such as Fuzzy Neural Network (FNN, \cite{han-why}), Exponential Smoothing Recurrent Neural Network (ESRNN, \cite{esrnn}), and Self-Organizing Radial Basis Function (SORBF, \cite{han-again-why})) and report better performance than others with fewer hidden neurons and less computational complexity. According to the authors, ``the proposed RSONN achieved better testing RMSE and running times than the other algorithms".

In April 2017, a group of researchers from Baidu research published \cite{sparse-rnn} in which they apply pruning to weights during the initial training of the network. At the end of the training, according to the authors, ``the parameters of the network are sparse while accuracy is still close to the original dense neural network". The authors claim ``the network size is reduced by $8\times$ and the time required to train the model remains constant". As explained in the paper, each recurrent layer has a constant number of hidden units, while we plan to have a different number of hidden units at each recurrent layer. All the experiments are also done on a private dataset, which prevents others from replicating the results, which is necessary to compare this technique of introducing sparsity with other approaches.

In November 2017, another group of researchers from Baidu research published \cite{narang-sparse} in which they propose a new pruning technique, block pruning. This technique can be used to zero out a block of weights during the training phase of a recurrent network, resulting in a Block-Sparse RNN. In addition to this, authors also use group lasso regularization to check if it can induce block sparsity. Authors report that ``block pruning and group lasso regularization with pruning are successful in creating block-sparse RNNs".  Their experiments report a nearly $10\times$ reduction in model size with a loss of 9\% to 17\% accuracy in vanilla RNN and GRU with block-sparsity of 4x4 blocks. Authors claim their approach is ``agnostic to the optimization" and it ``does not require any hyper-parameter retuning".

In September 2019, researchers from the University of Passau published \cite{julian} in which they aimed to predict the performance of Convolutional Neural Networks (CNNs) using its structural properties. Authors build Sparse Neural Networks (ANNs) by embedding Directed Acyclic Graphs (DAG) obtained through Random Graph Generators into Artificial Neural Networks. Using this approach, they create a dataset of 10000 such graphs, split them into the train-test set, and train it on the MNIST (\cite{mnist}) dataset. We will follow a similar approach to obtain arbitrarily structured Sparse Recurrent Neural Networks (Sparse-RNNs). Based on its performance, we will examine the importance of specific structural properties of the internal structure based on its impact on the performance of Sparse-RNNs.  This approach can also help in Neural Architectural Search (NAS) for RNNs using performance prediction as a tool.

In November 2019, Zhang et al. proposed a new one-shot pruning approach for Recurrent Neural Networks in \cite{zhang} obtained from the recurrent Jacobian spectrum. According to the authors, this technique works by ``forcing the network to preserve weights that propagate information through its temporal depths". Authors verified their approach with a GRU network on sequential MNIST (\cite{mnist}), Wikitext (\cite{wikitext}), and Billion words (\cite{billion_words}). Authors claim that ``At 95\% sparsity, our network achieves better results than fully dense networks, randomly pruned networks, SNIP (\cite{one_shot}) pruned networks, and Foresight (\cite{foresight}) pruned networks".

In December 2019, Wen et al. in \cite{wen} explain a different way of applying pruning to speedup Recurrent Neural Networks while avoiding the Lasso-based pruning methods. They introduced two sets of binary random variables to generate sparse masks for the weight matrix that, according to the authors, ``act as gates to the neurons". As explained in the paper, ``the presence of the matrix entry $w_{ij}$ depends on both the presence of the $i$-th input unit and the $j$-th output unit, while the value of $w_{ij}$ indicates the strength of the connection if $w_{ij} \neq 0$". The optimization of these variables is then performed by minimizing the $L_0$ norm of the weight matrix. This approach to language modeling and machine reading comprehension works comparatively to the state-of-the-art pruning approaches. Authors claim ``nearly 20$\times$ practical speedup during inference was achieved without losing performance for the language model on the Penn Treebank dataset". 

Now, after briefly acknowledging the research work done in terms of sparsity in RNNs, in the next chapter, we briefly explain the theoretical background of different approaches and algorithms that we will later use as part of our experiments.