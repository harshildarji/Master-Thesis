@ARTICLE{mnist,
  author={Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}}
  
@ARTICLE{Rosenblatt,
    author = {F. Rosenblatt},
    title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain},
    journal = {Psychological Review},
    year = {1958},
    pages = {65--386}
}

@article{mcculloch,
  added-at = {2008-02-26T11:58:58.000+0100},
  author = {Mcculloch, Warren and Pitts, Walter},
  biburl = {https://www.bibsonomy.org/bibtex/26fbacb0ae04bc17d296d9265dfc90dff/schaul},
  citeulike-article-id = {2380493},
  description = {idsia},
  interhash = {3e8e0d06f376f3eb95af89d5a2f15957},
  intrahash = {6fbacb0ae04bc17d296d9265dfc90dff},
  journal = {Bulletin of Mathematical Biophysics},
  keywords = {evolutionary},
  pages = {127--147},
  priority = {2},
  timestamp = {2008-02-26T12:00:58.000+0100},
  title = {A Logical Calculus of Ideas Immanent in Nervous Activity},
  volume = 5,
  year = 1943
}

@article{neocognitronbc,
  added-at = {2008-03-11T14:52:34.000+0100},
  author = {Fukushima, Kunihiko},
  biburl = {https://www.bibsonomy.org/bibtex/29ecd878c4827c46dab6b9622cfa00072/idsia},
  citeulike-article-id = {2376719},
  interhash = {303975e6400e477e91c91e7dc2c47544},
  intrahash = {9ecd878c4827c46dab6b9622cfa00072},
  journal = {Biological Cybernetics},
  keywords = {nn},
  pages = {193--202},
  priority = {2},
  timestamp = {2008-03-11T15:04:22.000+0100},
  title = {{N}eocognitron: {A} Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  volume = 36,
  year = 1980
}

@INPROCEEDINGS{7382560,  author={ {Meiyin Wu} and  {Li Chen}},  booktitle={2015 Chinese Automation Congress (CAC)},   title={Image recognition based on deep learning},   year={2015},  volume={},  number={},  pages={542-546},  doi={10.1109/CAC.2015.7382560}}

@INPROCEEDINGS{7822567,  author={Z. {Liang} and A. {Powell} and I. {Ersoy} and M. {Poostchi} and K. {Silamut} and K. {Palaniappan} and P. {Guo} and M. A. {Hossain} and A. {Sameer} and R. J. {Maude} and J. X. {Huang} and S. {Jaeger} and G. {Thoma}},  booktitle={2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},   title={CNN-based image analysis for malaria diagnosis},   year={2016},  volume={},  number={},  pages={493-496},  doi={10.1109/BIBM.2016.7822567}}

@book{goodfellow,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@ARTICLE{face_recon,  author={S. {Lawrence} and C. L. {Giles} and  {Ah Chung Tsoi} and A. D. {Back}},  journal={IEEE Transactions on Neural Networks},   title={Face recognition: a convolutional neural-network approach},   year={1997},  volume={8},  number={1},  pages={98-113},  doi={10.1109/72.554195}}

@article{backprop86,
  added-at = {2019-05-21T10:10:49.000+0200},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  biburl = {https://www.bibsonomy.org/bibtex/2a392597c4f9cff2cd3c96c2191fa1eb6/sxkdz},
  doi = {10.1038/323533a0},
  interhash = {c354bc293fa9aa7caffc66d40a014903},
  intrahash = {a392597c4f9cff2cd3c96c2191fa1eb6},
  journal = {Nature},
  keywords = {imported},
  number = 6088,
  pages = {533--536},
  timestamp = {2019-05-21T10:10:49.000+0200},
  title = {{Learning Representations by Back-propagating Errors}},
  url = {http://www.nature.com/articles/323533a0},
  volume = 323,
  year = 1986
}

@techreport{jordan,
  added-at = {2008-03-11T14:52:34.000+0100},
  author = {Jordan, Michael I.},
  biburl = {https://www.bibsonomy.org/bibtex/2c303b356b6262770f3aefc4a71bf8fcb/idsia},
  citeulike-article-id = {2377339},
  institution = {Institute for Cognitive Science, University of California, San Diego},
  interhash = {28982a74cc4ec26d2905f564f591c928},
  intrahash = {c303b356b6262770f3aefc4a71bf8fcb},
  keywords = {nn},
  number = 8604,
  priority = {2},
  timestamp = {2008-03-11T15:03:39.000+0100},
  title = {Serial Order: {A} Parallel, Distributed Processing Approach},
  year = 1986
}

@phdthesis{ilya,
author = {Sutskever, Ilya},
advisor = {Hinton, Geoffrey},
title = {Training Recurrent Neural Networks},
year = {2013},
isbn = {9780499220660},
publisher = {University of Toronto},
address = {CAN},
abstract = {Recurrent Neural Networks (RNNs) are powerful sequence models that were believed to be difficult to train, and as a result they were rarely used in machine learning applications. This thesis presents methods that overcome the difficulty of training RNNs, and applications of RNNs to challenging problems. We first describe a new probabilistic sequence model that combines Restricted Boltzmann Machines and RNNs. The new model is more powerful than similar models while being less difficult to train.Next, we present a new variant of the Hessian-free (HF) optimizer and show that it can train RNNs on tasks that have extreme long-range temporal dependencies, which were previously considered to be impossibly hard. We then apply HF to character-level language modelling and get excellent results. We also apply HF to optimal control and obtain RNN control laws that can successfully operate under conditions of delayed feedback and unknown disturbances.Finally, we describe a random parameter initialization scheme that allows gradient descent with momentum to train RNNs on problems with long-term dependencies. This directly contradicts widespread beliefs about the inability of first-order methods to do so, and suggests that previous attempts at training RNNs failed partly due to flaws in the random initialization.}
}

@inproceedings{deeprnn,
author = {Hermans, Michiel and Schrauwen, Benjamin},
title = {Training and Analyzing Deep Recurrent Neural Networks},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture. In this paper we study the effect of a hierarchy of recurrent neural networks on processing time series. Here, each layer is a recurrent network which receives the hidden state of the previous layer as input. This architecture allows us to perform hierarchical processing on difficult temporal tasks, and more naturally capture the structure of time series. We show that they reach state-of-the-art performance for recurrent networks in character-level language modeling when trained with simple stochastic gradient descent. We also offer an analysis of the different emergent time scales.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1},
pages = {190–198},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@inproceedings{nonlin,
author = {Su, Qinliang and Liao, Xuejun and Carin, Lawrence},
title = {A Probabilistic Framework for Nonlinearities in Stochastic Neural Networks},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a probabilistic framework for nonlinearities, based on doubly truncated Gaussian distributions. By setting the truncation points appropriately, we are able to generate various types of nonlinearities within a unified framework, including sigmoid, tanh and ReLU, the most commonly used nonlinearities in neural networks. The framework readily integrates into existing stochastic neural networks (with hidden units characterized as random variables), allowing one for the first time to learn the nonlinearities alongside model weights in these networks. Extensive experiments demonstrate the performance improvements brought about by the proposed framework when integrated with the restricted Boltzmann machine (RBM), temporal RBM and the truncated Gaussian graphical model (TGGM).},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4489–4498},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


@article{activation,
  author    = {Chigozie Nwankpa and
               Winifred Ijomah and
               Anthony Gachagan and
               Stephen Marshall},
  title     = {Activation Functions: Comparison of trends in Practice and Research
               for Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1811.03378},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.03378},
  archivePrefix = {arXiv},
  eprint    = {1811.03378},
  timestamp = {Fri, 23 Nov 2018 12:43:51 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-03378.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{tanh_1,
  title={Performance analysis of various activation functions in generalized MLP architectures of neural networks},
  author={Karlik, Bekir and Olgac, A Vehbi},
  journal={International Journal of Artificial Intelligence and Expert Systems},
  volume={1},
  number={4},
  pages={111--122},
  year={2011}
}

@article{tanh_2,
author = {Neal, Radford M.},
title = {Connectionist Learning of Belief Networks},
year = {1992},
issue_date = {July 1992},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {56},
number = {1},
issn = {0004-3702},
url = {https://doi.org/10.1016/0004-3702(92)90065-6},
doi = {10.1016/0004-3702(92)90065-6},
journal = {Artif. Intell.},
month = jul,
pages = {71–113},
numpages = {43}
}

@inproceedings{bptt-1,
  title={Backpropagation Through Time: What It Does and How to Do It},
  author={P. Werbos},
  year={1990}
}

@Article{lstm,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
}

@article{lstm_gates,
author = {Van Houdt, Greg and Mosquera, Carlos and Nápoles, Gonzalo},
year = {2020},
month = {12},
pages = {},
title = {A Review on the Long Short-Term Memory Model},
volume = {53},
journal = {Artificial Intelligence Review},
doi = {10.1007/s10462-020-09838-1}
}

@inproceedings{lstm_time,
author = {Siami Namini, Sima and Tavakoli, Neda and Siami Namin, Akbar},
year = {2018},
month = {12},
pages = {1394-1401},
title = {A Comparison of ARIMA and LSTM in Forecasting Time Series},
doi = {10.1109/ICMLA.2018.00227}
}

@article{lstm_drug,
author = {Gupta, Anvita and Müller, Alex T. and Huisman, Berend J. H. and Fuchs, Jens A. and Schneider, Petra and Schneider, Gisbert},
title = {Generative Recurrent Networks for De Novo Drug Design},
journal = {Molecular Informatics},
volume = {37},
number = {1-2},
pages = {1700111},
keywords = {Chemogenomics, deep learning, drug discovery, machine learning, medicinal chemistry},
doi = {https://doi.org/10.1002/minf.201700111},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/minf.201700111},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/minf.201700111},
abstract = {Abstract Generative artificial intelligence models present a fresh approach to chemogenomics and de novo drug design, as they provide researchers with the ability to narrow down their search of the chemical space and focus on regions of interest. We present a method for molecular de novo design that utilizes generative recurrent neural networks (RNN) containing long short-term memory (LSTM) cells. This computational model captured the syntax of molecular representation in terms of SMILES strings with close to perfect accuracy. The learned pattern probabilities can be used for de novo SMILES generation. This molecular design concept eliminates the need for virtual compound library enumeration. By employing transfer learning, we fine-tuned the RNN′s predictions for specific molecular targets. This approach enables virtual compound design without requiring secondary or external activity prediction, which could introduce error or unwanted bias. The results obtained advocate this generative RNN-LSTM system for high-impact use cases, such as low-data drug discovery, fragment based molecular design, and hit-to-lead optimization for diverse drug targets.},
year = {2018}
}

@InProceedings{lstm_music,
author="Eck, Douglas
and Schmidhuber, J{\"u}rgen",
editor="Dorronsoro, Jos{\'e} R.",
title="Learning the Long-Term Structure of the Blues",
booktitle="Artificial Neural Networks --- ICANN 2002",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="284--289",
abstract="In general music composed by recurrent neural networks (RNNs) suffers from a lack of global structure. Though networks can learn note-by-note transition probabilities and even reproduce phrases, they have been unable to learn an entire musical form and use that knowledge to guide composition. In this study, we describe model details and present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and some listeners believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure it does not drift from it: LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen.",
isbn="978-3-540-46084-8"
}

@article{gru,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@inproceedings{gru_gates,
  title={Deep gate recurrent neural network},
  author={Gao, Yuan and Glowacka, Dorota},
  booktitle={Asian conference on machine learning},
  pages={350--365},
  year={2016}
}

@InProceedings{gru_water_level,
author="Le, Xuan-Hien
and Ho, Hung Viet
and Lee, Giha",
editor="Trung Viet, Nguyen
and Xiping, Dou
and Thanh Tung, Tran",
title="Application of Gated Recurrent Unit (GRU) Network for Forecasting River Water Levels Affected by Tides",
booktitle="APAC 2019",
year="2020",
publisher="Springer Singapore",
address="Singapore",
pages="673--680",
abstract="In light of the proliferation of information technology, the application of deep learning models in the analysis and study of hydrological problems is increasingly becoming common. This paper proposes a new approach using one of the applications of deep learning models to predict river water level in areas where the influence of tides is obvious. The forecasting model is developed based on the recurrent neural network for predicting the water level from one to four time-steps ahead in the downstream of An Tho irrigation culvert on the Luoc River (Vietnam). Each time-step corresponds to the once observed data and the data collected for this study is only the observed water level at the target station - An Tho sluice in over 18 years. Although only a modest amount of data is required, the forecasting model produces superior results. Accuracy in the phase of testing the model is up to 94-96{\%} for all forecasting cases. The findings of this study indicate that the proposed model produces an outstanding performance when the target-forecasting station is clearly affected by the tide. This acts as a precursor of the construction of an operating regime for irrigation sluice gates in the tidal area.",
isbn="978-981-15-0291-0"
}

@ARTICLE{gru_speech,  author={M. {Ravanelli} and P. {Brakel} and M. {Omologo} and Y. {Bengio}},  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},   title={Light Gated Recurrent Units for Speech Recognition},   year={2018},  volume={2},  number={2},  pages={92-102},  doi={10.1109/TETCI.2017.2762739}}

@INPROCEEDINGS{gru_lstm,  author={S. {Yang} and X. {Yu} and Y. {Zhou}},  booktitle={2020 International Workshop on Electronic Communication and Artificial Intelligence (IWECAI)},   title={LSTM and GRU Neural Network Performance Comparison Study: Taking Yelp Review Dataset as an Example},   year={2020},  volume={},  number={},  pages={98-101},  doi={10.1109/IWECAI50956.2020.00027}}

@INPROCEEDINGS{deep_res,  author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}},  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Deep Residual Learning for Image Recognition},   year={2016},  volume={},  number={},  pages={770-778},  doi={10.1109/CVPR.2016.90}}

@article{sparse_nn,
  author    = {Simon Alford and
               Ryan A. Robinett and
               Lauren Milechin and
               Jeremy Kepner},
  title     = {Pruned and Structurally Sparse Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1810.00299},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.00299},
  archivePrefix = {arXiv},
  eprint    = {1810.00299},
  timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-00299.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mao,
  title={Exploring the regularity of sparse structure in convolutional neural networks},
  author={Mao, Huizi and Han, Song and Pool, Jeff and Li, Wenshuo and Liu, Xingyu and Wang, Yu and Dally, William J},
  journal={arXiv preprint arXiv:1705.08922},
  year={2017}
}

@inproceedings{sparse_face,
  title={Sparsifying neural network connections for face recognition},
  author={Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4856--4864},
  year={2016}
}

@article{julian,
  author    = {Julian Stier and
               Michael Granitzer},
  title     = {Structural Analysis of Sparse Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1910.07225},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.07225},
  archivePrefix = {arXiv},
  eprint    = {1910.07225},
  timestamp = {Tue, 22 Oct 2019 18:17:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-07225.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lee_pruning,
  title={Pruning recurrent neural networks for improved generalization performance},
  author={Giles, C Lee and Omlin, Christian W},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={5},
  pages={848--851},
  year={1994},
  publisher={IEEE}
}

@article{han_pruning,
author = {Han, Hong-Gui and Zhang, Shuo and Qiao, Jun-Fei},
title = {An Adaptive Growing and Pruning Algorithm for Designing Recurrent Neural Network},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {242},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.02.038},
doi = {10.1016/j.neucom.2017.02.038},
abstract = {The training of recurrent neural networks (RNNs) concerns the selection of their structures and the connection weights. To efficiently enhance generalization capabilities of RNNs, a recurrent self-organizing neural networks (RSONN), using an adaptive growing and pruning algorithm (AGPA), is proposed for improving their performance in this paper. This AGPA can self-organize the structures of RNNs based on the information processing ability and competitiveness of hidden neurons in the learning process. Then, the hidden neurons of RSONN can be added or pruned to improve the generalization performance. Furthermore, an adaptive second-order algorithm with adaptive learning rate is employed to adjust the parameters of RSONN. And the convergence of RSONN is given to show the computational efficiency. To demonstrate the merits of RSONN for data modeling, several benchmark datasets and a real world application associated with nonlinear systems modeling problems are examined with comparisons against other existing methods. Experimental results show that the proposed RSONN effectively simplifies the network structure and performs better than some exiting methods.},
journal = {Neurocomput.},
month = jun,
pages = {51–62},
numpages = {12},
keywords = {Convergence, Competitiveness, Adaptive growing and pruning algorithm, Information processing ability, Recurrent self-organizing neural network}
}

@article{han-why,
  title={A fuzzy neural network approach for online fault detection in waste water treatment process},
  author={Honggui, Han and Ying, Li and Junfei, Qiao},
  journal={Computers \& Electrical Engineering},
  volume={40},
  number={7},
  pages={2216--2226},
  year={2014},
  publisher={Elsevier}
}

@article{esrnn,
  title={A developmental approach to structural self-organization in reservoir computing},
  author={Yin, Jun and Meng, Yan and Jin, Yaochu},
  journal={IEEE transactions on autonomous mental development},
  volume={4},
  number={4},
  pages={273--289},
  year={2012},
  publisher={IEEE}
}

@article{sparse-rnn,
  author    = {Sharan Narang and
               Gregory F. Diamos and
               Shubho Sengupta and
               Erich Elsen},
  title     = {Exploring Sparsity in Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1704.05119},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.05119},
  archivePrefix = {arXiv},
  eprint    = {1704.05119},
  timestamp = {Mon, 13 Aug 2018 16:46:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/NarangDSE17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{han-again-why,
  title={Prediction of activated sludge bulking based on a self-organizing RBF neural network},
  author={Han, Hong-Gui and Qiao, Jun-Fei},
  journal={Journal of Process Control},
  volume={22},
  number={6},
  pages={1103--1112},
  year={2012},
  publisher={Elsevier}
}

@article{narang-sparse,
  title={Block-sparse recurrent neural networks},
  author={Narang, Sharan and Undersander, Eric and Diamos, Gregory},
  journal={arXiv preprint arXiv:1711.02782},
  year={2017}
}

@article{zhang,
  author    = {Matthew Shunshi Zhang and
               Bradly Stadie},
  title     = {One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum
               Evaluation},
  journal   = {CoRR},
  volume    = {abs/1912.00120},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.00120},
  archivePrefix = {arXiv},
  eprint    = {1912.00120},
  timestamp = {Thu, 02 Jan 2020 18:08:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-00120.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wikitext,
  author    = {Stephen Merity and
               Caiming Xiong and
               James Bradbury and
               Richard Socher},
  title     = {Pointer Sentinel Mixture Models},
  journal   = {CoRR},
  volume    = {abs/1609.07843},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.07843},
  archivePrefix = {arXiv},
  eprint    = {1609.07843},
  timestamp = {Thu, 21 Mar 2019 11:19:44 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/MerityXBS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@techreport{billion_words,
title	= {One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling},
author	= {Ciprian Chelba and Tomas Mikolov and Mike Schuster and Qi Ge and Thorsten Brants and Phillipp Koehn and Tony Robinson},
year	= {2013},
URL	= {http://arxiv.org/abs/1312.3005},
institution	= {Google}
}

@article{one_shot,
  title={Snip: Single-shot network pruning based on connection sensitivity},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  journal={arXiv preprint arXiv:1810.02340},
  year={2018}
}

@misc{foresight,
      title={Picking Winning Tickets Before Training by Preserving Gradient Flow}, 
      author={Chaoqi Wang and Guodong Zhang and Roger Grosse},
      year={2020},
      eprint={2002.07376},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{wen,
  title={Structured pruning of recurrent neural networks through neuron selection},
  author={Wen, Liangjian and Zhang, Xuanyang and Bai, Haoli and Xu, Zenglin},
  journal={Neural Networks},
  volume={123},
  pages={134--141},
  year={2020},
  publisher={Elsevier}
}

@article{reber,
  title={Implicit learning of synthetic languages: The role of instructional set.},
  author={Reber, Arthur S},
  journal={Journal of Experimental Psychology: Human Learning and Memory},
  volume={2},
  number={1},
  pages={88},
  year={1976},
  publisher={American Psychological Association}
}

@misc{russian,
  abstract = {Neural machine translation is a recently proposed approach to machine
translation. Unlike the traditional statistical machine translation, the neural
machine translation aims at building a single neural network that can be
jointly tuned to maximize the translation performance. The models proposed
recently for neural machine translation often belong to a family of
encoder-decoders and consists of an encoder that encodes a source sentence into
a fixed-length vector from which a decoder generates a translation. In this
paper, we conjecture that the use of a fixed-length vector is a bottleneck in
improving the performance of this basic encoder-decoder architecture, and
propose to extend this by allowing a model to automatically (soft-)search for
parts of a source sentence that are relevant to predicting a target word,
without having to form these parts as a hard segment explicitly. With this new
approach, we achieve a translation performance comparable to the existing
state-of-the-art phrase-based system on the task of English-to-French
translation. Furthermore, qualitative analysis reveals that the
(soft-)alignments found by the model agree well with our intuition.},
  added-at = {2020-06-07T20:24:58.000+0200},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  biburl = {https://www.bibsonomy.org/bibtex/2713375898fd7d2477f6ab6dc3dd66c2c/jan.hofmann1},
  description = {[1409.0473] Neural Machine Translation by Jointly Learning to Align and Translate},
  interhash = {bb2ca011eeafccb0bd2505c9476dcd10},
  intrahash = {713375898fd7d2477f6ab6dc3dd66c2c},
  keywords = {thema:pyramid_scene_parsing},
  note = {cite arxiv:1409.0473 Comment: Accepted at ICLR 2015 as oral presentation},
  timestamp = {2020-06-07T20:24:58.000+0200},
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  url = {http://arxiv.org/abs/1409.0473},
  year = 2014
}

@article{recommender,
title = "A hybrid recommender system using artificial neural networks",
journal = "Expert Systems with Applications",
volume = "83",
pages = "300 - 313",
year = "2017",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2017.04.046",
url = "http://www.sciencedirect.com/science/article/pii/S0957417417302968",
author = "Tulasi K. Paradarami and Nathaniel D. Bastian and Jennifer L. Wightman",
keywords = "Information retrieval, Artificial neural networks, Recommender systems, Supervised learning",
abstract = "In the context of recommendation systems, metadata information from reviews written for businesses has rarely been considered in traditional systems developed using content-based and collaborative filtering approaches. Collaborative filtering and content-based filtering are popular memory-based methods for recommending new products to the users but suffer from some limitations and fail to provide effective recommendations in many situations. In this paper, we present a deep learning neural network framework that utilizes reviews in addition to content-based features to generate model based predictions for the business-user combinations. We show that a set of content and collaborative features allows for the development of a neural network model with the goal of minimizing logloss and rating misclassification error using stochastic gradient descent optimization algorithm. We empirically show that the hybrid approach is a very promising solution when compared to standalone memory-based collaborative filtering method."
}

@article{image_recon,
title = "Deep convolution neural network for image recognition",
journal = "Ecological Informatics",
volume = "48",
pages = "257 - 268",
year = "2018",
issn = "1574-9541",
doi = "https://doi.org/10.1016/j.ecoinf.2018.10.002",
url = "http://www.sciencedirect.com/science/article/pii/S1574954118302140",
author = "Boukaye Boubacar Traore and Bernard Kamsu-Foguem and Fana Tangara",
keywords = "Deep learning, Convolution neural networks, Pathogen images classification, Smart microscope",
abstract = "During an epidemic crisis, medical image analysis namely microscopic analyses are made to confirm or not the existence of the epidemic pathogen in suspected cases. Pathogen are all infectious agents such as a virus, bacterium, protozoa, prion etc. However, there is often a lack of specialists in the handling of microscopes, hence allowing the need to make the microscopic analysis abroad. This results in a considerable loss of time and in the meantime, the epidemic continues to spread. To save time in the analysis of samples, we propose to make the future microscopes more intelligent so that they will be able to indicate by themselves the existence or not of the pathogen of an epidemic in a sample. To have a smart microscope, we propose a methodology based on efficient Convolution Neural Network (CNN) architecture in order to classify epidemic pathogen with five deep learning phases: (1) Training dataset of provided images (2) CNN Training (3) Testing data preparation (4) CNN generated model on testing data and finally (5) Evaluation of images classified. The resulted classification process can be integrated in a mobile computing solution on future microscopes. CNN can improve the accuracy in pathogens diagnosis that are focused on hand-tuned feature extraction implying some human mistakes. For our study, we consider cholera and malaria epidemics for microscopic images classification with a relevant CNN, respectively Vibrio cholerae images and Plasmodium falciparum images. Image classification is the task of taking an input image and outputting a class or a probability of classes that best describes the image. Interesting results have been obtained from the CNN model generated achieving the classification accuracy of 94\%, with 200 Vibrio cholera images and 200 Plasmodium falciparum images for training dataset and 80 images for testing data. Although this document addresses the classification of epidemic pathogen images using a CNN model, the underlying principles apply to the other fields of science and technology, because of its performance and its capability to handle more layers than the previous traditional neural networks."
}

@ARTICLE{dey,  author={S. {Dey} and K. {Huang} and P. A. {Beerel} and K. M. {Chugg}},  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},   title={Pre-Defined Sparse Neural Networks With Hardware Acceleration},   year={2019},  volume={9},  number={2},  pages={332-345},  doi={10.1109/JETCAS.2019.2910864}}

@InProceedings{liu,
author = {Liu, Baoyuan and Wang, Min and Foroosh, Hassan and Tappen, Marshall and Pensky, Marianna},
title = {Sparse Convolutional Neural Networks},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {06},
year = {2015}
}

@online{perc_eq,
  author = {Raschka, Sebastian},
  title = {Single-Layer Neural Networks and Gradient Descent},
  year = 2015,
  url = {https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html}
}

@online{sigmoid,
  author = {Restrepo, Ronny},
  title = {Derivative of the Sigmoid function - a worked example},
  year = 2017,
  url = {http://ronny.rest/blog/post_2017_08_10_sigmoid/}
}

@BOOK{tanh,
  TITLE = {Deep Learning By Example},
  AUTHOR = {Menshawy, Ahmed},
  YEAR = {2018},
  PUBLISHER = {Packt Publishing},
}

@misc{blalock,
      title={What is the State of Neural Network Pruning?}, 
      author={Davis Blalock and Jose Javier Gonzalez Ortiz and Jonathan Frankle and John Guttag},
      year={2020},
      eprint={2003.03033},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{li,
  title={Optimization based Layer-wise Magnitude-based Pruning for DNN Compression.},
  author={Li, Guiying and Qian, Chao and Jiang, Chunhui and Lu, Xiaofen and Tang, Ke},
  booktitle={IJCAI},
  volume={1},
  pages={2},
  year={2018}
}

@article{hall,
author = {Hall, Lawrence O. and Bowyer, Kevin W. and Banfield, Robert E. and Eschrich, Steven and Collins, Richard},
title = {Is Error-Based Pruning Redeemable?},
journal = {International Journal on Artificial Intelligence Tools},
volume = {12},
number = {03},
pages = {249-264},
year = {2003},
doi = {10.1142/S0218213003001228},

URL = { 
        https://doi.org/10.1142/S0218213003001228
    
},
eprint = { 
        https://doi.org/10.1142/S0218213003001228
    
}
,
    abstract = { Error based pruning can be used to prune a decision tree and it does not require the use of validation data. It is implemented in the widely used C4.5 decision tree software. It uses a parameter, the certainty factor, that affects the size of the pruned tree. Several researchers have compared error based pruning with other approaches, and have shown results that suggest that error based pruning results in larger trees that give no increase in accuracy. They further suggest that as more data is added to the training set, the tree size after applying error based pruning continues to grow even though there is no increase in accuracy. It appears that these results were obtained with the default certainty factor value. Here, we show that varying the certainty factor allows significantly smaller trees to be obtained with minimal or no accuracy loss. Also, the growth of tree size with added data can be halted with an appropriate choice of certainty factor. Methods of determining the certainty factor are discussed for both small and large data sets. Experimental results support the conclusion that error based pruning can be used to produce appropriately sized trees with good accuracy when compared with reduced error pruning. }
}

@INPROCEEDINGS{hall_1,
  author={L. O. {Hall} and R. {Collins} and K. W. {Bowyer} and R. {Banfield}},
  booktitle={14th IEEE International Conference on Tools with Artificial Intelligence, 2002. (ICTAI 2002). Proceedings.}, 
  title={Error-based pruning of decision trees grown on very large data sets can work!}, 
  year={2002},
  volume={},
  number={},
  pages={233-238},
  doi={10.1109/TAI.2002.1180809}}
  
@article{carlson,
author = {Carlson, Stephan C.},
title = {Graph theory},
publisher={Encyclopædia Britannica},
month= {11},
year = {2020},
URL = {https://www.britannica.com/topic/graph-theory},
}

@ARTICLE{porter,
AUTHOR = {Porter, M. A.},
TITLE   = {{S}mall-world network},
YEAR    = {2012},
JOURNAL = {Scholarpedia},
VOLUME  = {7},
NUMBER  = {2},
PAGES   = {1739},
DOI     = {10.4249/scholarpedia.1739},
NOTE    = {revision \#187873}
}

@Inbook{schreiber,
author="Schreiber, Falk",
editor="Dubitzky, Werner
and Wolkenhauer, Olaf
and Cho, Kwang-Hyun
and Yokota, Hiroki",
title="Characteristic Path Length",
bookTitle="Encyclopedia of Systems Biology",
year="2013",
publisher="Springer New York",
address="New York, NY",
pages="395--395",
isbn="978-1-4419-9863-7",
doi="10.1007/978-1-4419-9863-7_1460",
url="https://doi.org/10.1007/978-1-4419-9863-7_1460"
}

@article{watts,
  title={Collective dynamics of ‘small-world’networks},
  author={Watts, Duncan J and Strogatz, Steven H},
  journal={nature},
  volume={393},
  number={6684},
  pages={440--442},
  year={1998},
  publisher={Nature Publishing Group}
}

@INPROCEEDINGS{aarstad,  author={J. {Aarstad} and H. {Ness} and S. A. {Haugland}},  booktitle={2013 IEEE International Conference on Industrial Technology (ICIT)},   title={In what ways are small-world and scale-free networks interrelated?},   year={2013},  volume={},  number={},  pages={1483-1487},  doi={10.1109/ICIT.2013.6505891}}

@misc{lattice,
      title={Lattice Graph}, 
      author={Weisstein, Eric W},
      publisher={MathWorld--A Wolfram Web Resource},
      url={https://mathworld.wolfram.com/LatticeGraph.html}
}

@article{pytorch,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@inproceedings{tensorflow,
title	= {TensorFlow: A system for large-scale machine learning},
author	= {Martin Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
year	= {2016},
URL	= {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
booktitle	= {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
pages	= {265--283}
}

